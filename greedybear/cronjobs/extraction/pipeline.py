import logging
from collections import defaultdict

from greedybear.cronjobs.extraction.strategies.factory import ExtractionStrategyFactory
from greedybear.cronjobs.repositories import (
    ElasticRepository,
    IocRepository,
    SensorRepository,
)
from greedybear.cronjobs.scoring.scoring_jobs import UpdateScores
from greedybear.settings import (
    EXTRACTION_INTERVAL,
    INITIAL_EXTRACTION_TIMESPAN,
)


class ExtractionPipeline:
    """
    Pipeline for extracting IOCs from T-Pot's honeypot logs.
    Orchestrates the extraction workflow.
    """

    def __init__(self):
        """Initialize the pipeline with required repositories."""
        self.log = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.elastic_repo = ElasticRepository()
        self.ioc_repo = IocRepository()
        self.sensor_repo = SensorRepository()

    @property
    def _minutes_back_to_lookup(self) -> int:
        """
        Calculate the time window size for Elasticsearch queries.
        Returns a larger window on first run to backfill historical data,
        otherwise uses the configured extraction interval.

        Returns:
            Number of minutes to look back in the search query.
        """
        if self.ioc_repo.is_empty():
            return INITIAL_EXTRACTION_TIMESPAN
        return EXTRACTION_INTERVAL

    def execute(self) -> int:
        """
        Execute the extraction pipeline.

        Performs the following steps:
        1. Search Elasticsearch for honeypot log entries in chunks
        2. For each chunk, group hits by honeypot type and extract sensors
        3. Apply honeypot-specific extraction strategies
        4. Update IOC scores

        Returns:
            Number of IOC records processed.
        """
        ioc_record_count = 0
        factory = ExtractionStrategyFactory(self.ioc_repo, self.sensor_repo)

        # 1. Search in chunks
        self.log.info("Getting honeypot hits from Elasticsearch")
        for chunk in self.elastic_repo.search(self._minutes_back_to_lookup):
            ioc_records = []
            hits_by_honeypot = defaultdict(list)

            # 2. Group by honeypot
            self.log.info("Grouping hits by honeypot type")
            for hit in chunk:
                # skip hits with non-existing or empty sources
                if "src_ip" not in hit or not hit["src_ip"].strip():
                    continue
                # skip hits with non-existing or empty types (=honeypots)
                if "type" not in hit or not hit["type"].strip():
                    continue
                # extract sensor
                if "t-pot_ip_ext" in hit:
                    # Extract sensor location from geoip_ext
                    geoip_ext_data = hit.get("geoip_ext", {})
                    sensor_country_name = geoip_ext_data.get("country_name")

                    self.sensor_repo.add_sensor(hit["t-pot_ip_ext"], sensor_country_name)
                hits_by_honeypot[hit["type"]].append(hit.to_dict())

            # 3. Extract using strategies
            for honeypot, hits in sorted(hits_by_honeypot.items()):
                if not self.ioc_repo.is_ready_for_extraction(honeypot):
                    self.log.info(f"Skipping honeypot {honeypot}")
                    continue
                self.log.info(f"Extracting hits from honeypot {honeypot}")
                strategy = factory.get_strategy(honeypot)
                try:
                    strategy.extract_from_hits(hits)
                    ioc_records += strategy.ioc_records
                except Exception as exc:
                    self.log.error(f"Extraction failed for honeypot {honeypot}: {exc}")

            # 4. Update scores
            self.log.info("Updating scores")
            if ioc_records:
                UpdateScores().score_only(ioc_records)
            ioc_record_count += len(ioc_records)

        return ioc_record_count
